{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1904754d-3f66-4c13-8886-481d805fd196",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ee326-f6ca-472b-bf70-0d889a50b27c",
   "metadata": {},
   "source": [
    "In this exercice we will use a **pre-trained convnet** as a feature extractor. \\\n",
    "We will **finetune the convnet** to solve a classification problem\n",
    "\n",
    "This notebooks is heavily inpired by : https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4954ca2-dc6e-48ea-b891-8dc997b2e9ee",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb4d16-1539-4adf-a120-f42f800a5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "!pip install wget\n",
    "import wget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100 \n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba07fcb-83f3-4353-a99f-87dde5776f61",
   "metadata": {},
   "source": [
    "## Prepare PyTorch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbed29-c7e2-43c4-82d7-df2d1562b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(device)\n",
    "print(f\"Running notebook on {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586f0be-f5ad-43aa-8b99-6dcdde55ddfd",
   "metadata": {},
   "source": [
    "## CIFAR-100 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfaa86-14f6-44a1-b60a-ea2b3998edab",
   "metadata": {},
   "source": [
    "Download the cifar-100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420ce2e-5a07-4c48-b5b3-1314c39402d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = CIFAR100(\"_downloaded\", train=True, download=True, transform=ToTensor())\n",
    "num_samples = len(dataset)\n",
    "num_train = int(num_samples * 0.7)\n",
    "num_valid = int(num_samples * 0.2)\n",
    "num_test = num_samples - num_valid - num_train\n",
    "lengths = [num_train, num_valid, num_test]\n",
    "\n",
    "# Split data into train/valid/test dataset\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "\n",
    "dataset_sizes = {}\n",
    "dataset_sizes['train'] = len(train_dataset)\n",
    "dataset_sizes['val'] = len(valid_dataset)\n",
    "dataset_sizes['test'] = len(test_dataset)\n",
    "\n",
    "# Train Dataloader\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = DataLoader(train_dataset,\n",
    "                                  batch_size=32,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=8)\n",
    "\n",
    "# Valid Dataloader\n",
    "dataloaders['val'] = DataLoader(valid_dataset,\n",
    "                                  batch_size=32,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=8)\n",
    "\n",
    "# Test Dataloader\n",
    "dataloaders['test'] = DataLoader(test_dataset,\n",
    "                                 batch_size=32,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=8)\n",
    "\n",
    "# Statistics\n",
    "print(\"training examples : \", dataset_sizes['train'])\n",
    "print(\"validation examples : \", dataset_sizes['val'])\n",
    "print(\"test examples :\", dataset_sizes['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bd2cb-7b0c-4083-aece-11f4fe13522b",
   "metadata": {},
   "source": [
    "## Visualize input (labels + images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4fa14-141b-4417-8765-c1cc509d424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', \n",
    "                  'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', \n",
    "                  'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', \n",
    "                  'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "                  'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', \n",
    "                  'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', \n",
    "                  'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', \n",
    "                  'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', \n",
    "                  'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', \n",
    "                  'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', \n",
    "                  'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
    "\n",
    "def show(data, cols = 10, rows = 4):\n",
    "    figure = plt.figure(figsize=(16, 8))\n",
    "    to_PIL = ToPILImage()\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(data), size=(1,)).item()\n",
    "        img, label = data[sample_idx]                \n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(category_names[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(to_PIL(img), cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "show(train_dataset, cols = 10, rows = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc4c7f-6302-4e48-92ea-35d8a5b60e15",
   "metadata": {},
   "source": [
    "## Pre-trained ConvNet (ResNet18) as a fixed feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffe33b-e157-47e7-acb2-d813a94ffaa1",
   "metadata": {},
   "source": [
    "Use ResNet18 [https://arxiv.org/abs/1512.03385] has a base\n",
    "The output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7a3b5-ae97-4b0b-a8f1-3559305740e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CNN model\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "#print(model_conv) # Uncomment to visualize the model layers\n",
    "\n",
    "# Disable the gradient computation for all the network\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last layer (model_conv.fc) to match our categories (from ImageNet with 1000 categories to CIFAR 100)\n",
    "num_features = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_features, len(category_names))\n",
    "model_conv.fc.requires_grad = True\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "trainable_parameters = sum(p.numel() for p in model_conv.parameters() if p.requires_grad)\n",
    "print(f\"{trainable_parameters} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4375a-2720-4134-9cfc-ba8cd0ebe385",
   "metadata": {},
   "source": [
    "## Visualize the filter (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b1571-b3dd-437f-bdb4-733efc29fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - visualize the filter and feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d4ed0-e4d8-4574-a035-9a9ae245ff60",
   "metadata": {},
   "source": [
    "## Optimize ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7b1f6-70a4-4b82-900c-c6bbda041468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from : https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "# This code is a standard pytorch training loop and unrelated to the transfer learning logic\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad0b58-c03c-4436-b854-c75369840146",
   "metadata": {},
   "source": [
    "Define the model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc7bd7-bbe1-4918-9647-5741776412f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = model_conv.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad285c-c40f-4e68-a544-0db5685068e1",
   "metadata": {},
   "source": [
    "Run the training (**be patient ! It took 11m 18s on )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b06619-80ec-404f-a54c-09b337639b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n",
    "# TODO - save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72296c-c4ea-43e5-82b5-ee9a01d2cb24",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd3356-d30d-4d74-813d-b40b6d4ac0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b6152-aa75-4645-ace1-235e4a2210f3",
   "metadata": {},
   "source": [
    "## The Exercice ! Explore ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecc811-2c6f-4623-9cd5-99ca2446e225",
   "metadata": {},
   "source": [
    "Experimentations in Machine Learning is essential. For this part you will experiment with couple of \n",
    "- different CNN models (ResNet, MobileNets, ...)\n",
    "- different transfer (we are using the last layer, what about keep the first few conv ? )\n",
    "- different hyperparameters (batch size, learning rate)\n",
    "\n",
    "The goal is to report your result to the ML Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb53b9-d578-4a41-8e20-8b16821b7e26",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b16ad3-66b1-4838-843e-4a87aee2ef94",
   "metadata": {},
   "source": [
    "- \"Deep Residual Learning for Image Recognition\", 2015 by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun [https://arxiv.org/abs/1512.03385]\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f681f21-f602-404c-9c0a-56adba7b097d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
